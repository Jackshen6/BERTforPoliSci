{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to successfully complete this assignment you need to commit this report to your project git repository on or before **11:59pm on Friday March 13**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Using Machine Learning (ML) in Political Science</center>\n",
    "\n",
    "<center>by Nikolaos Frantzeskakis</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, machine learning has proven to be a valuable tool for political scientiests. Notable examples include classifying party affiliation and political opinion (Shulman 2005; Yu et al. 2008). It has also been especially useful in text analysis tasks (Grimmer and Stewart 2013; Lucas et al. 2015), and particularly in text classification (Hopkins and King 2008; Clayton et al. 2017).\n",
    "\n",
    "Advanced text classifiers that have been used in political science research projects up to this point combine SVMs with the logic and assumptions of a bag-of-words. A bag-of-words is in essence a list of all the words that exist in a text, often followed by the number of times that they appear. Several algorithms have been developed to make use of this information by “learning” which words appear more frequently in every class. Following that, they can classify the new texts. In many cases the above approach yielded great results. The main weakness of this method, though, is the volume of information discarded. A bag-of-words model can analyze texts only in the word level. This has a number of repercussions. First, the syntax is not taken into consideration. Second, for the list to work, words need to be stemmed, which further decreases the complexity of the text as well as the nuances possible in the text. Third, words that did not appear in the training texts are routinely discarded if found in the new texts. In essence, for a bag-of-words model to be effective, language needs to be sufficient simple and finite. \n",
    "\n",
    "My research focuses a lot on text analysis and text classificiation. As a result I machine learning is a key tool for me. However, I try to use tools developed in computer science that are not yet being widely used in political science. In order to avoid the pitfalls involved in the use of bag-of-words models, in my own work I use of word embeddings instead of the bag-of-words, which is novel in political science. Word embeddings map words in a continuous numeric vector. Words with similar meanings are mapped closer to each other. Therefore, it becomes possible for the model to “learn” more efficiently by being able to find which words have similar (or opposite) meanings via vector arithmetic. For example, the relationship between king and queen could be described as follows: king – man + woman = queen (Rudkowsky et al. 2018). Based on this different logic my models are capable of analyzing text in sentences rather than words, which means that they have the potential to handle complex bodies of text and produce results much closer to those produced by a human coder.\n",
    "\n",
    "In addition, I use neural networks instead of SVMs as they have great potential in netuarl language processing tasks. In my current projects I employ the pre-trained network BERT, which is made available by Google (Devlin 2018). BERT is to the best of my knowledge the most advanced publicly available neural netowork trained for NLP tasks. It became available about a year ago, at which point it comfortably broke a large number of records on tasks like text categorization and next sentence prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# References\n",
    "\n",
    "Clayton, Amanda, Cecilia Josefsson, and Vibeke Wang. \"Quotas and women's substantive representation: Evidence from a content analysis of Ugandan plenary debates.\" Politics & Gender 13.2 (2017): 276-304.\n",
    "\n",
    "Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" *arXiv preprint arXiv:1810.04805* (2018).\n",
    "\n",
    "Grimmer, Justin, and Brandon M. Stewart. \"Text as data: The promise and pitfalls of automatic content analysis methods for political texts.\" Political analysis 21.3 (2013): 267-297.\n",
    "\n",
    "Hopkins, Daniel J., and Gary King. \"A method of automated nonparametric content analysis for social science.\" American Journal of Political Science 54.1 (2010): 229-247.\n",
    "\n",
    "Lucas, Christopher, et al. \"Computer-assisted text analysis for comparative politics.\" Political Analysis 23.2 (2015): 254-277.\n",
    "\n",
    "Rudkowsky, Elena, et al. \"More than bags of words: Sentiment analysis with word embeddings.\" Communication Methods and Measures 12.2-3 (2018): 140-157.\n",
    "\n",
    "Shulman, Stuart W. \"E-rulemaking: Issues in current research and practice.\" International Journal of Public Administration 28.7-8 (2005): 621-641.\n",
    "\n",
    "Yu, Bei, Stefan Kaufmann, and Daniel Diermeier. \"Classifying party affiliation from political speech.\" Journal of Information Technology & Politics 5.1 (2008): 33-48."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
